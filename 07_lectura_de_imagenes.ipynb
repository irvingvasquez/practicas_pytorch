{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de imágenes\n",
    "\n",
    "En este notebook veremos como introducir imágenes a nuestra red neuronal.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Existen multiples formas para hacerlo, pero en este tutorial utilizaremos la función *dataset.ImageFolder* incorporada en el paquete *torchvision*. La forma rápida de leer un conjunto de imágenes es\n",
    "\n",
    "```python\n",
    "dataset = datasets.ImageFolder('directorio/', transform=transformacion)\n",
    "```\n",
    "\n",
    "la función *ImageFolder* espera que dentro del directorio los archivos esté organizados por folderes, donde cada folder contiene las imagenes de una clase en específico.\n",
    "\n",
    "En este ejemplo usaré una base de datos propia que contiene dos clases, cactus y no_cactus. La base de datos tiene dos carpetas con los nombres *cactus* y *no_cactus*.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/practicas_pytorch/blob/master/07_lectura_de_imagenes.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos paquetes\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos que usaremos será el de cactus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para linux\n",
    "#!wget https://jivg.org/wp-content/uploads/2024/07/cactus_course_dataset.zip\n",
    "\n",
    "# para windows\n",
    "!curl -o cactus_course_dataset.zip https://jivg.org/wp-content/uploads/2024/07/cactus_course_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para linux\n",
    "#!unzip cactus_course_dataset.zip\n",
    "\n",
    "# Para windows\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"cactus_course_dataset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"cactus_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones\n",
    "\n",
    "Usualmente las imagenes que leas estarán en un tamaño diferente al que necesitas para la red, asi que es necesario convertirlas a un formato adecuado, además será necesario convertirlas a tensores de pytorch. Todo esto lo podemos hacer con la función *transforms.Compose()*, en la cual podemos apilar las transformaciones necesarias. Por ejemplo,\n",
    "\n",
    "```python\n",
    "transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorio de la carpeta\n",
    "directorio = 'cactus_dataset/cactus_course_dataset/training_set/'\n",
    "\n",
    "# aplicaré una serie de transformaciones\n",
    "# 1. escalar las imágenes a 32 x 32 pixeles\n",
    "# 2. convertir a tensores\n",
    "transformaciones = transforms.Compose([transforms.Resize(32),\n",
    "                               transforms.CenterCrop(32),\n",
    "                               transforms.ToTensor()])  \n",
    "datos = datasets.ImageFolder(directorio, transformaciones) \n",
    "\n",
    "print(f\"Number of classes: {len(datos.classes)}\")\n",
    "print(f\"Classes: {datos.classes}\")\n",
    "print(f\"Number of images: {len(datos)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "Image folder se encarga de las imágenes, sin embargo todavía es necesario otro objeto, el *DataLoader* quien se encarga de leer los ejemplos por lotes (batches) junto con su correspondiente etiqueta. En el objeto *DataLoader* puedes especificar diversos parametros como el tamaño del lote si los datos son mezclados (*suffled*) despúes de cada época, entre otras opciones. Por ejemplo,\n",
    "\n",
    "```python\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "Para iterar sobre los datos podemos hacer:\n",
    "\n",
    "```python\n",
    "# Looping through it, get a batch on each loop \n",
    "for images, labels in dataloader:\n",
    "    pass\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(dataloader))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear el objeto dataloader\n",
    "cargador = torch.utils.data.DataLoader(datos, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to test your data loader\n",
    "images, labels = next(iter(cargador))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(9):\n",
    "    img = images[i]\n",
    "    label = labels[i]\n",
    "    print(label)\n",
    "    helper.imshow(img, ax=axes[i], normalize=False)\n",
    "    axes[i].set_title(f'Class: {label}')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "# TODO: Define transforms for the training data and testing data\n",
    "train_transforms = \n",
    "\n",
    "test_transforms = \n",
    "\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the trainloader or testloader \n",
    "data_iter = iter(testloader)\n",
    "\n",
    "images, labels = next(data_iter)\n",
    "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
    "for ii in range(4):\n",
    "    ax = axes[ii]\n",
    "    helper.imshow(images[ii], ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
