{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Convolucional\n",
    "\n",
    "Una red neuronal convolucional (CNN por sus siglas en inglés) es un tipo de de red profunda en la que se comparten los pesos en cada capa. Una de las redes colvolucionales más famosas es LeNet5 de Yan LeCun [1]. En este notebook implementaremos la red neuronal convolucional LeNet5 con PyTorch para clasificar tipos de ropa.\n",
    "\n",
    "Primero hay que notar la aquitectura de la red. Ésta se compone de una parte convolucional y otra parte completamente conectada. La parte convolucional incluye dos capas con maxpooling y la completamente conectada se compone de dos capas ocultas y una de salida.\n",
    "\n",
    "@juan1rving\n",
    "\n",
    "<img src=\"archivos/lenet.png\">\n",
    "\n",
    "[1] LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos paquetes\n",
    "# Cargamos paquetes necesarios\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#helper was developed by Udacity under MIT license\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiper-parámetros\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo usaremos el conjunto de datos [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist). Fashion-MNIST es un conjunto de datos desarrollado por Zalado que contiene imagenes de ropa asociadas con 10 clases. El conjunto contiene 60,000 ejemplos de entrenamiento y 10,000 para validación. Cada imagen tiene una resolución de 28x28 en un solo canal. Este conjunto comparte la misma estructura y tamaño con el conjunto de datos de MNIST.\n",
    "\n",
    "A continuación crearemos los objetos Dataset y Dataloader que se encargarán del manejo del conjunto. Además mostraremos algunas estadísticas y ejemplos en el conjunto.\n",
    "\n",
    "[2] Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una transformación de los datos\n",
    "transform = transforms.Compose([transforms.Resize(32), # escalar a 32\n",
    "                                transforms.ToTensor(), # convertir a tensores\n",
    "                                transforms.Normalize([0.5], [0.5])]) #normalizar a media y desv std\n",
    "\n",
    "# Descargamos el conjunto de entrenamiento y cargamos mediante un dataLoader\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True)\n",
    "\n",
    "# Descargamos el conjunto de validación\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size, shuffle=True)\n",
    "\n",
    "# Imprimir información estadística del conjunto de datos\n",
    "print('Train data, number of images: ', len(trainset))\n",
    "print('Test data, number of images: ', len(testset))\n",
    "\n",
    "# Nombrar las clases de acuerdo al índice que tienen\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener un lote de ejemplos\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy()\n",
    "\n",
    "display_size = 10\n",
    "\n",
    "# Graficar los ejemplos junto a las clases que le corresponden\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(display_size):\n",
    "    ax = fig.add_subplot(2, display_size, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la RNC\n",
    "Si estudiamos la arquitectura obsevaremos que esta compuesta por 2 capas convolucionales, despúes de cada convolución se realiza un max pooling. Se aplana el mapa de características y se agrega una red completamente conectada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de LeNet5\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, n_clases):\n",
    "        '''\n",
    "        Construimos la estructura de LeNet5\n",
    "        \n",
    "        '''\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # De acuerdo al artículo de LeCun La primera capa está compuesta por 6 kernels de 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 1 canal de entrada 6 feature maps de salida, kernels de 5x5\n",
    "        \n",
    "        # Después tenemos una capa maxpooling\n",
    "        # kernel_size=2, stride=2\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Agregamos otra capa convolucional con 16 kernels de 5 x 5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Maxpooling\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Capas totalmente conectadas\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, n_clases)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Definimos el pase frontal (forward pass)\n",
    "        '''\n",
    "        # Agregamos los ReLUs\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        #print(x.size())\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # prep for linear layer by flattening the feature maps into feature vectors\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # capas lineales\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y validación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar la red\n",
    "model = LeNet5(10)\n",
    "# Definir la función de costo (critetio de optimización)\n",
    "criterion = nn.NLLLoss()\n",
    "# Instanciar optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información de la red\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos una función de evaluación\n",
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "        #images.resize_(images.shape[0], 784)\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del entrenamiento y validación\n",
    "\n",
    "En esta sección implementamos el algoritmo de descenso por gradiente y haremos el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Antes del descenso por gradiente y el entrenamiento \n",
    "# verificaremos que la red está bien implementada a través verificar la exactitud que tiene sin haber sido entrenada.\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate through test dataset\n",
    "for images, labels in testloader:\n",
    "    \n",
    "    # warp input images in a Variable wrapper\n",
    "    images = Variable(images)\n",
    "\n",
    "    # forward pass to get outputs\n",
    "    # the outputs are a series of class scores\n",
    "    outputs = model(images)\n",
    "\n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # count up total number of correct labels\n",
    "    # for which the predicted and true labels are equal\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = 100 * correct.numpy() / total\n",
    "\n",
    "# print it out!\n",
    "print('Porcentaje de exactitud antes de entrenar: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Hiperparámetros\n",
    "\n",
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 40\n",
    "\n",
    "\n",
    "# Descenso por gradiente\n",
    "for e in range(epochs):\n",
    "    # Cambiamos a modo entrenamiento\n",
    "    model.train()\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        steps += 1\n",
    "        \n",
    "        # Aplanar imágenes a un vector de 784 elementos\n",
    "        # images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # wrap them in a torch Variable\n",
    "        images, labels = Variable(images), Variable(labels)       \n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Optimización\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Cambiamos a modo de evaluación\n",
    "            model.eval()\n",
    "            \n",
    "            # Apagamos los gradientes, reduce memoria y cálculos\n",
    "            with torch.no_grad():\n",
    "                test_loss, accuracy = validation(model, testloader, criterion)\n",
    "                \n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure training is back on\n",
    "            model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar los resultados obtenidos\n",
    "\n",
    "A continuación mostraremos algunos ejemplos de la inferencia que está realizando la red una vez entrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "display_size = 10\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "# get predictions\n",
    "preds = np.squeeze(model(Variable(images, volatile=True)).data.max(1, keepdim=True)[1].numpy())\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(display_size):\n",
    "    ax = fig.add_subplot(2, display_size, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicas_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e22d029f5570ef7df543599926afc42bb090457ba5a887f8aae20fd6018d0da0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
