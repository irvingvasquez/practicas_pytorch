{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de funciones torch.nn\n",
    "\n",
    "El módulo torch.nn en PyTorch contiene un amplio conjunto de clases y funciones que facilitan la construcción y el entrenamiento de redes neuronales. A continuación usaremos varias funciones como práctica.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/practicas_pytorch/blob/master/01_2_funciones.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de ReLU\n",
    "\n",
    "La función ReLU (Rectified Linear Unit) es una de las funciones de activación más populares en las redes neuronales, especialmente en las redes neuronales profundas. Su popularidad se debe a su simplicidad y efectividad en resolver algunos problemas comunes en el entrenamiento de redes neuronales, como el problema del desvanecimiento del gradiente.\n",
    "\n",
    "$$z = max(0, x)$$\n",
    "\n",
    "Si bien la función ReLU ya la podemos encontrar implementada en PyTorch,\n",
    "\n",
    "```python\n",
    "torch.nn.functional.relu(tensor)\n",
    "```\n",
    "\n",
    "La idea de este ejercicio es familiarizarnos con las funciones de PyTorch y su funcionamiento. Por ejemplo, podemos hacer uso de las funciones:\n",
    "\n",
    "```python\n",
    "torch.maximum()\n",
    "torch.zeros()\n",
    "torch.exp()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementa la función de activaciòn ReLU\n",
    "def ReLU(tensor):\n",
    "  return torch.maximum(torch.zeros(tensor.shape), tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 0.0000],\n",
       "        [2.5000, 0.0000, 6.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar el funcionamiento de la función ReLU\n",
    "a = torch.tensor([[1, 2, -3], [2.5, -0.2, 6]])\n",
    "ReLU(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
